{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPKozC76VFNV"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn tqdm transformers torch --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UK4pevnV3HA",
        "outputId": "1b1ebbc4-342b-46d8-c043-d951ca632727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement numpy==1.21.6 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4, 1.25.0, 1.25.1, 1.25.2, 1.26.0, 1.26.1, 1.26.2, 1.26.3, 1.26.4, 2.0.0, 2.0.1, 2.0.2, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.2.4, 2.2.5, 2.2.6, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.3.5)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for numpy==1.21.6\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.21.6 scipy==1.7.3 scikit-learn==1.1.3 scikit-multilearn==0.2.0 tqdm --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xs73jxSPV5RR",
        "outputId": "79abe965-d4ca-4ca9-c415-5ac905806729"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.3.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy) (2.3.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy --upgrade\n",
        "!pip install scipy --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqWwYKDsV7Me"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import urllib.request\n",
        "import bz2\n",
        "from tqdm import tqdm\n",
        "\n",
        "from scipy.sparse import vstack  # only for final train+val stacking if needed\n",
        "from sklearn.metrics import f1_score, hamming_loss\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.neighbors import NearestNeighbors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMhXgl_lV9OH",
        "outputId": "470c4fc8-7685-460c-d340-5fd24cf8f101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 1000, Val: 200, Test: 200\n"
          ]
        }
      ],
      "source": [
        "# Download and read LexGLUE ECtHR (B) raw text dataset\n",
        "base = \"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel/\"\n",
        "files_b = {\n",
        "    \"train\": \"ecthr_b_lexglue_raw_texts_train.txt.bz2\",\n",
        "    \"val\":   \"ecthr_b_lexglue_raw_texts_val.txt.bz2\",\n",
        "    \"test\":  \"ecthr_b_lexglue_raw_texts_test.txt.bz2\"\n",
        "}\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "def download_if_missing(fname):\n",
        "    dest = os.path.join(\"data\", fname)\n",
        "    if not os.path.exists(dest):\n",
        "        url = base + fname\n",
        "        print(\"Downloading:\", url)\n",
        "        urllib.request.urlretrieve(url, dest)\n",
        "    return dest\n",
        "\n",
        "def read_raw_txt_bz2(path):\n",
        "    texts, labels = [], []\n",
        "    with bz2.open(path, \"rt\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            if \"\\t\" in line:\n",
        "                labstr, txt = line.split(\"\\t\", 1)\n",
        "            else:\n",
        "                parts = line.split(maxsplit=1)\n",
        "                if len(parts) < 2:\n",
        "                    continue\n",
        "                labstr, txt = parts\n",
        "            labs = [l.strip() for l in labstr.split(\",\") if l.strip() != \"\"]\n",
        "            labels.append(labs)\n",
        "            texts.append(txt.replace(\"\\n\", \" \"))\n",
        "    return texts, labels\n",
        "\n",
        "# Download all files for ECtHR (B)\n",
        "paths_b = {k: download_if_missing(v) for k, v in files_b.items()}\n",
        "\n",
        "# Read them\n",
        "train_texts_b, train_labels_b = read_raw_txt_bz2(paths_b[\"train\"])\n",
        "val_texts_b, val_labels_b     = read_raw_txt_bz2(paths_b[\"val\"])\n",
        "test_texts_b, test_labels_b   = read_raw_txt_bz2(paths_b[\"test\"])\n",
        "\n",
        "train_texts_b = train_texts_b[:1000]\n",
        "train_labels_b = train_labels_b[:1000]\n",
        "\n",
        "val_texts_b = val_texts_b[:200]\n",
        "val_labels_b = val_labels_b[:200]\n",
        "\n",
        "test_texts_b = test_texts_b[:200]\n",
        "test_labels_b = test_labels_b[:200]\n",
        "\n",
        "print(f\"Train: {len(train_texts_b)}, Val: {len(val_texts_b)}, Test: {len(test_texts_b)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CSla_lAV_v1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e50b3681-a6da-4163-fdab-b3b07520d30c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing train set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 63/63 [00:40<00:00,  1.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing validation set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:07<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:06<00:00,  1.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes:\n",
            "Train embeddings: (1000, 200)\n",
            "Train targets: (1000, 109)\n",
            "Val embeddings: (200, 200)\n",
            "Val targets: (200, 109)\n",
            "Test embeddings: (200, 200)\n",
            "Test targets: (200, 109)\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "# 1. Load LEGAL-BERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# 2. Setup batching\n",
        "def get_cls_embeddings(texts, batch_size=16):\n",
        "    all_embeddings = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size)):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer(\n",
        "            batch,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        )\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            cls_embeddings = outputs.pooler_output.detach().cpu().numpy()\n",
        "            all_embeddings.append(cls_embeddings)\n",
        "    return np.vstack(all_embeddings)  # Shape: (num_samples, 768)\n",
        "\n",
        "# 3. Encode multi-label targets\n",
        "mlb = MultiLabelBinarizer()\n",
        "mlb.fit(train_labels_b + val_labels_b + test_labels_b)  # Fit on all splits\n",
        "\n",
        "# 4. Prepare each split (Example: Train set)\n",
        "print(\"Processing train set...\")\n",
        "train_embeddings = get_cls_embeddings(train_texts_b, batch_size=16)\n",
        "train_targets = mlb.transform(train_labels_b)  # Shape: (num_samples, num_classes)\n",
        "\n",
        "print(\"Processing validation set...\")\n",
        "val_embeddings = get_cls_embeddings(val_texts_b, batch_size=16)\n",
        "val_targets = mlb.transform(val_labels_b)\n",
        "\n",
        "print(\"Processing test set...\")\n",
        "test_embeddings = get_cls_embeddings(test_texts_b, batch_size=16)\n",
        "test_targets = mlb.transform(test_labels_b)\n",
        "\n",
        "pca = PCA(n_components=200)\n",
        "train_embeddings_pca = pca.fit_transform(train_embeddings)\n",
        "val_embeddings_pca = pca.transform(val_embeddings)\n",
        "test_embeddings_pca = pca.transform(test_embeddings)\n",
        "\n",
        "print(\"Shapes:\")\n",
        "print(\"Train embeddings:\", train_embeddings_pca.shape)\n",
        "print(\"Train targets:\", train_targets.shape)\n",
        "print(\"Val embeddings:\", val_embeddings_pca.shape)\n",
        "print(\"Val targets:\", val_targets.shape)\n",
        "print(\"Test embeddings:\", test_embeddings_pca.shape)\n",
        "print(\"Test targets:\", test_targets.shape)\n",
        "\n",
        "# Now you have (embeddings, targets) pairs for ML modeling!\n",
        "# Example for torch/keras: X = train_embeddings, y = train_targets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jL4WOGykWC1f"
      },
      "outputs": [],
      "source": [
        "def ContinousCrossover(posA, posB, gamma=0.4):\n",
        "    alpha = np.random.uniform(-gamma, 1 + gamma, size=posA.shape)\n",
        "    child1 = alpha * posA + (1 - alpha) * posB\n",
        "    child2 = alpha * posB + (1 - alpha) * posA\n",
        "    return child1, child2\n",
        "\n",
        "def ContinousMutation(position, problem, sigma=0.1):\n",
        "    VarMin = problem['VarMin']\n",
        "    VarMax = problem['VarMax']\n",
        "    noise = np.random.normal(scale=sigma, size=position.shape)\n",
        "    mutated = position + noise\n",
        "    return np.clip(mutated, VarMin, VarMax)\n",
        "\n",
        "def printingperiter(problem, it, gbest, namemethod, funcevals, curtrial, population=None):\n",
        "    print(f\"[{namemethod}] Iter {it} | Evals {funcevals} | Best cost {gbest['cost']:.5f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKBxJn82WE5i"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "def rf_classifier_cost_function(particle, problem, pos=None):\n",
        "    \"\"\"\n",
        "    Cost function for RandomForest, used by the Mayfly optimizer.\n",
        "    We now optimize:\n",
        "      - pos[0]: log10(n_estimators)  -> integer 10 to 1000\n",
        "      - pos[1]: max_depth            -> integer 2 to 20\n",
        "      - pos[2]: min_samples_split    -> integer 2 to 20\n",
        "    Cost = 1 - F1_micro(validation)\n",
        "    \"\"\"\n",
        "    if pos is None:\n",
        "        pos = np.array(particle['position']).flatten()\n",
        "\n",
        "    # Decode hyperparameters\n",
        "    log10_n_estimators = float(pos[0])\n",
        "    max_depth_cont     = float(pos[1])\n",
        "    mss_cont           = float(pos[2])\n",
        "\n",
        "    # n_estimators\n",
        "    n_estimators = int(np.round(10 ** log10_n_estimators))\n",
        "    n_estimators = int(np.clip(n_estimators, 10, 1000))\n",
        "\n",
        "    # max_depth\n",
        "    max_depth = int(np.round(np.clip(max_depth_cont, 2, 20)))\n",
        "\n",
        "    # min_samples_split\n",
        "    min_samples_split = int(np.round(np.clip(mss_cont, 2, 20)))\n",
        "\n",
        "    try:\n",
        "        base_rf = RandomForestClassifier(\n",
        "            n_estimators=n_estimators,\n",
        "            max_depth=max_depth,\n",
        "            min_samples_split=min_samples_split,\n",
        "            n_jobs=-1,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        clf = MultiOutputClassifier(base_rf)\n",
        "\n",
        "        clf.fit(problem['Xtrain'], problem['Ytrain'])\n",
        "        preds = clf.predict(problem['Xval'])\n",
        "\n",
        "        f1_micro = f1_score(problem['Yval'], preds, average='micro', zero_division=0)\n",
        "        cost = 1.0 - f1_micro  # lower is better\n",
        "\n",
        "        particle['model'] = clf\n",
        "        particle['hyperparams'] = {\n",
        "            'n_estimators': n_estimators,\n",
        "            'max_depth': max_depth,\n",
        "            'min_samples_split': min_samples_split\n",
        "        }\n",
        "        particle['cost'] = cost\n",
        "\n",
        "        return particle\n",
        "    except Exception as e:\n",
        "        print('Error with RandomForest:', e)\n",
        "        particle['cost'] = np.inf\n",
        "        return particle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bYCR_4UWHDC"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def MA(problem, IterPrint, MaxIter, MaxFuncEvals, curtrial, initialpop,\n",
        "       mPopSize, fPopSize, a1=1.0, a2=1.5, a3=1.5, beta=2, dance=5, fl=1,\n",
        "       dance_damp=0.8, fl_damp=0.99, nc=10, gmax=0.8, gmin=0.8, gamma=0.4):\n",
        "\n",
        "    namemethod = 'MA'\n",
        "    VarMin = problem['VarMin']\n",
        "    VarMax = problem['VarMax']\n",
        "    nVar = problem['nVar']\n",
        "    VelMax = 0.1 * (VarMax - VarMin)\n",
        "    VelMin = -VelMax\n",
        "    g = gmax\n",
        "    funcevals = -1\n",
        "    start_time = time.time()\n",
        "\n",
        "    convergence_cost, convergence_f1 = [], []\n",
        "    CostFunction = problem['CostFunction']\n",
        "    gbest = {'position': None, 'cost': np.inf}\n",
        "\n",
        "    # --- initialize ---\n",
        "    pop, popf = [], []\n",
        "    for i in range(mPopSize):\n",
        "        pos = initialpop[i]['position']\n",
        "        particle = {'position': pos, 'velocity': np.zeros(nVar)}\n",
        "        particle = CostFunction(particle, problem)\n",
        "        particle['best_position'] = pos.copy()\n",
        "        particle['best_cost'] = particle['cost']\n",
        "        if particle['cost'] < gbest['cost']:\n",
        "            gbest = particle.copy()\n",
        "        pop.append(particle)\n",
        "        funcevals += 1\n",
        "\n",
        "    for i in range(fPopSize):\n",
        "        pos = initialpop[i + mPopSize]['position']\n",
        "        particle = {'position': pos, 'velocity': np.zeros(nVar)}\n",
        "        particle = CostFunction(particle, problem)\n",
        "        if particle['cost'] < gbest['cost']:\n",
        "            gbest = particle.copy()\n",
        "        popf.append(particle)\n",
        "        funcevals += 1\n",
        "\n",
        "    it = 0\n",
        "    while funcevals < MaxFuncEvals - 1 and it < MaxIter:\n",
        "        it += 1\n",
        "\n",
        "        # --- update females ---\n",
        "        for i in range(fPopSize):\n",
        "            if popf[i]['cost'] > pop[i]['cost']:\n",
        "                rmf = abs(pop[i]['position'] - popf[i]['position'])\n",
        "                popf[i]['velocity'] = g*popf[i]['velocity'] + a3*np.exp(-beta * rmf**2)*(pop[i]['position'] - popf[i]['position'])\n",
        "            else:\n",
        "                popf[i]['velocity'] = g*popf[i]['velocity'] + fl*np.random.uniform(-1,1,size=(nVar))\n",
        "            popf[i]['velocity'] = np.clip(popf[i]['velocity'], VelMin, VelMax)\n",
        "            popf[i]['position'] = np.clip(popf[i]['position'] + popf[i]['velocity'], VarMin, VarMax)\n",
        "            popf[i] = CostFunction(popf[i], problem)\n",
        "            funcevals += 1\n",
        "            if popf[i]['cost'] < gbest['cost']:\n",
        "                gbest = popf[i].copy()\n",
        "\n",
        "        # --- update males ---\n",
        "        for i in range(mPopSize):\n",
        "            if pop[i]['cost'] > gbest['cost']:\n",
        "                rpbest = abs(pop[i]['best_position'] - pop[i]['position'])\n",
        "                rgbest = abs(gbest['position'] - pop[i]['position'])\n",
        "                pop[i]['velocity'] = (g*pop[i]['velocity']\n",
        "                    + a1*np.exp(-beta*rpbest**2)*(pop[i]['best_position'] - pop[i]['position'])\n",
        "                    + a2*np.exp(-beta*rgbest**2)*(gbest['position'] - pop[i]['position']))\n",
        "            else:\n",
        "                pop[i]['velocity'] = g*pop[i]['velocity'] + dance*np.random.uniform(-1,1,size=(nVar))\n",
        "            pop[i]['velocity'] = np.clip(pop[i]['velocity'], VelMin, VelMax)\n",
        "            pop[i]['position'] = np.clip(pop[i]['position'] + pop[i]['velocity'], VarMin, VarMax)\n",
        "            pop[i] = CostFunction(pop[i], problem)\n",
        "            funcevals += 1\n",
        "            if pop[i]['cost'] < pop[i]['best_cost']:\n",
        "                pop[i]['best_position'] = pop[i]['position'].copy()\n",
        "                pop[i]['best_cost'] = pop[i]['cost']\n",
        "                if pop[i]['best_cost'] < gbest['cost']:\n",
        "                    gbest = pop[i].copy()\n",
        "\n",
        "        # --- parameter update ---\n",
        "        g = gmax - ((gmax - gmin) / MaxFuncEvals) * funcevals\n",
        "        dance *= dance_damp\n",
        "        fl *= fl_damp\n",
        "\n",
        "        # --- Log per iteration ---\n",
        "        history_iter.append(it)\n",
        "\n",
        "        pos = np.array(gbest['position']).flatten()\n",
        "        history_log10_n_estimators.append(float(pos[0]))\n",
        "        history_max_depth.append(float(pos[1]))\n",
        "        history_min_samples_split.append(float(pos[2]))\n",
        "        history_f1micro.append(1.0 - gbest['cost'])\n",
        "        history_iter.append(it)\n",
        "\n",
        "\n",
        "        if it % IterPrint == 0:\n",
        "            printingperiter(problem, it, gbest, namemethod, funcevals, curtrial)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    return namemethod, gbest['position'], gbest['cost'], elapsed_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDyMPjyZWJJq"
      },
      "outputs": [],
      "source": [
        "# We now optimize 3 hyperparameters:\n",
        "#   pos[0]: log10(n_estimators)     -> n_estimators in [10^1, 10^3] = 10–1000\n",
        "#   pos[1]: max_depth               -> [2, 20]\n",
        "#   pos[2]: min_samples_split       -> [2, 20]\n",
        "\n",
        "dim = 3\n",
        "\n",
        "VarMin = np.array([1.0, 2.0, 2.0])    # lower bounds\n",
        "VarMax = np.array([3.0, 20.0, 20.0])  # upper bounds\n",
        "\n",
        "problem = {\n",
        "    'VarMin': VarMin,\n",
        "    'VarMax': VarMax,\n",
        "    'nVar': dim,\n",
        "    'CostFunction': rf_classifier_cost_function,\n",
        "    'Xtrain': train_embeddings_pca,\n",
        "    'Ytrain': train_targets,\n",
        "    'Xval': val_embeddings_pca,\n",
        "    'Yval': val_targets,\n",
        "    'StopCriterion': 'Iterations'\n",
        "}\n",
        "\n",
        "# Initial population\n",
        "mPopSize = 25\n",
        "fPopSize = 25\n",
        "initialpop = []\n",
        "for _ in range(mPopSize + fPopSize):\n",
        "    pos = VarMin + (VarMax - VarMin) * np.random.rand(dim)\n",
        "    initialpop.append({'position': pos})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lLL5wGPWfcZ"
      },
      "outputs": [],
      "source": [
        "# GLOBAL TRACKERS for RandomForest Mayfly\n",
        "history_log10_n_estimators = []  # pos[0]\n",
        "history_max_depth = []           # pos[1]\n",
        "history_min_samples_split = []   # pos[2]\n",
        "history_f1micro = []\n",
        "history_iter = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PeAm5cAWy-a",
        "outputId": "27365195-38ea-404b-e772-2e5aaf4b3a50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kept 96 labels (removed 13 constant ones).\n",
            "Single XGBoost fit took: 22.406083345413208 seconds\n",
            "Single XGBoost fit took: 29.29612398147583 seconds\n",
            "Single XGBoost fit took: 35.78815150260925 seconds\n",
            "Single XGBoost fit took: 29.29738450050354 seconds\n",
            "Single XGBoost fit took: 13.125570058822632 seconds\n",
            "Single XGBoost fit took: 33.92222809791565 seconds\n",
            "Single XGBoost fit took: 12.065368175506592 seconds\n",
            "Single XGBoost fit took: 11.553357601165771 seconds\n",
            "Single XGBoost fit took: 30.46051287651062 seconds\n",
            "Single XGBoost fit took: 28.372154474258423 seconds\n",
            "Single XGBoost fit took: 28.39618158340454 seconds\n",
            "Single XGBoost fit took: 12.038315057754517 seconds\n",
            "Single XGBoost fit took: 12.23536229133606 seconds\n",
            "Single XGBoost fit took: 11.682385206222534 seconds\n",
            "Single XGBoost fit took: 29.453702926635742 seconds\n",
            "Single XGBoost fit took: 29.157683849334717 seconds\n",
            "Single XGBoost fit took: 28.228788137435913 seconds\n",
            "Single XGBoost fit took: 11.783010005950928 seconds\n",
            "Single XGBoost fit took: 17.827211618423462 seconds\n",
            "Single XGBoost fit took: 28.920130252838135 seconds\n",
            "Single XGBoost fit took: 29.427425384521484 seconds\n",
            "Single XGBoost fit took: 29.41629457473755 seconds\n",
            "Single XGBoost fit took: 12.495602369308472 seconds\n",
            "Single XGBoost fit took: 28.59744620323181 seconds\n",
            "[MA] Iter 1 | Evals 23 | Best cost 0.81673\n",
            "Single XGBoost fit took: 12.794867753982544 seconds\n",
            "Single XGBoost fit took: 13.273104906082153 seconds\n",
            "Single XGBoost fit took: 25.143839836120605 seconds\n",
            "Single XGBoost fit took: 30.564067125320435 seconds\n",
            "Single XGBoost fit took: 27.973313808441162 seconds\n",
            "Single XGBoost fit took: 12.527318954467773 seconds\n",
            "Single XGBoost fit took: 13.964107751846313 seconds\n",
            "Single XGBoost fit took: 29.757813215255737 seconds\n",
            "Single XGBoost fit took: 28.991714239120483 seconds\n",
            "Single XGBoost fit took: 29.19438672065735 seconds\n",
            "Single XGBoost fit took: 13.085402727127075 seconds\n",
            "Single XGBoost fit took: 27.42845606803894 seconds\n",
            "[MA] Iter 2 | Evals 35 | Best cost 0.80083\n",
            "Single XGBoost fit took: 12.472323417663574 seconds\n",
            "Single XGBoost fit took: 13.139383792877197 seconds\n",
            "Single XGBoost fit took: 19.998844385147095 seconds\n",
            "Single XGBoost fit took: 29.932249069213867 seconds\n",
            "Single XGBoost fit took: 28.313213348388672 seconds\n",
            "Single XGBoost fit took: 14.83009648323059 seconds\n",
            "Single XGBoost fit took: 11.963231325149536 seconds\n",
            "Single XGBoost fit took: 29.590857982635498 seconds\n",
            "Single XGBoost fit took: 28.92047095298767 seconds\n",
            "Single XGBoost fit took: 28.89047336578369 seconds\n",
            "Single XGBoost fit took: 12.138282537460327 seconds\n",
            "Single XGBoost fit took: 26.7045578956604 seconds\n",
            "[MA] Iter 3 | Evals 47 | Best cost 0.80083\n",
            "Single XGBoost fit took: 11.755568742752075 seconds\n",
            "Single XGBoost fit took: 14.16487741470337 seconds\n",
            "Single XGBoost fit took: 24.98531675338745 seconds\n",
            "Single XGBoost fit took: 29.640120267868042 seconds\n",
            "Single XGBoost fit took: 29.010959148406982 seconds\n",
            "Single XGBoost fit took: 17.8405179977417 seconds\n",
            "Single XGBoost fit took: 12.656150102615356 seconds\n",
            "Single XGBoost fit took: 29.629714727401733 seconds\n",
            "Single XGBoost fit took: 28.89207434654236 seconds\n",
            "Single XGBoost fit took: 27.505030393600464 seconds\n",
            "Single XGBoost fit took: 12.087712287902832 seconds\n",
            "Single XGBoost fit took: 26.25446057319641 seconds\n",
            "[MA] Iter 4 | Evals 59 | Best cost 0.80083\n",
            "Single XGBoost fit took: 11.773021221160889 seconds\n",
            "Single XGBoost fit took: 12.31699538230896 seconds\n",
            "Single XGBoost fit took: 29.581525087356567 seconds\n",
            "Single XGBoost fit took: 29.91895079612732 seconds\n",
            "Single XGBoost fit took: 28.746748208999634 seconds\n",
            "Single XGBoost fit took: 15.228227138519287 seconds\n",
            "Single XGBoost fit took: 14.115074872970581 seconds\n",
            "Single XGBoost fit took: 29.354855060577393 seconds\n",
            "Single XGBoost fit took: 30.163600206375122 seconds\n",
            "Single XGBoost fit took: 27.517027854919434 seconds\n",
            "Single XGBoost fit took: 12.470263481140137 seconds\n",
            "Single XGBoost fit took: 24.016570806503296 seconds\n",
            "[MA] Iter 5 | Evals 71 | Best cost 0.80083\n",
            "Single XGBoost fit took: 13.155133962631226 seconds\n",
            "Single XGBoost fit took: 11.802288055419922 seconds\n",
            "Single XGBoost fit took: 29.980592012405396 seconds\n",
            "Single XGBoost fit took: 29.548635721206665 seconds\n",
            "Single XGBoost fit took: 28.14277172088623 seconds\n",
            "Single XGBoost fit took: 17.57258367538452 seconds\n",
            "Single XGBoost fit took: 13.275843858718872 seconds\n",
            "Single XGBoost fit took: 28.867581367492676 seconds\n",
            "Single XGBoost fit took: 28.92224383354187 seconds\n",
            "Single XGBoost fit took: 28.610291719436646 seconds\n",
            "Single XGBoost fit took: 12.44937515258789 seconds\n",
            "Single XGBoost fit took: 18.817638635635376 seconds\n",
            "[MA] Iter 6 | Evals 83 | Best cost 0.80083\n",
            "Single XGBoost fit took: 14.019678831100464 seconds\n",
            "Single XGBoost fit took: 12.725332498550415 seconds\n",
            "Single XGBoost fit took: 30.326666355133057 seconds\n",
            "Single XGBoost fit took: 24.790724515914917 seconds\n",
            "Single XGBoost fit took: 28.267204999923706 seconds\n",
            "Single XGBoost fit took: 14.365589380264282 seconds\n",
            "Single XGBoost fit took: 12.49290657043457 seconds\n",
            "Single XGBoost fit took: 29.905226945877075 seconds\n",
            "Single XGBoost fit took: 29.237810850143433 seconds\n",
            "Single XGBoost fit took: 27.667592525482178 seconds\n",
            "Single XGBoost fit took: 12.358557939529419 seconds\n",
            "Single XGBoost fit took: 15.991451501846313 seconds\n",
            "[MA] Iter 7 | Evals 95 | Best cost 0.80083\n",
            "Single XGBoost fit took: 12.426829814910889 seconds\n",
            "Single XGBoost fit took: 14.960154294967651 seconds\n",
            "Single XGBoost fit took: 30.160871505737305 seconds\n",
            "Single XGBoost fit took: 26.267939567565918 seconds\n",
            "Single XGBoost fit took: 28.42504358291626 seconds\n",
            "Single XGBoost fit took: 14.623302221298218 seconds\n",
            "Single XGBoost fit took: 13.373005867004395 seconds\n",
            "Single XGBoost fit took: 29.896475076675415 seconds\n",
            "Single XGBoost fit took: 29.06462550163269 seconds\n",
            "Single XGBoost fit took: 25.858402013778687 seconds\n",
            "Single XGBoost fit took: 12.62211275100708 seconds\n",
            "Single XGBoost fit took: 12.839636087417603 seconds\n",
            "[MA] Iter 8 | Evals 107 | Best cost 0.80083\n",
            "Single XGBoost fit took: 11.791646242141724 seconds\n",
            "Single XGBoost fit took: 12.754277467727661 seconds\n",
            "Single XGBoost fit took: 30.601930618286133 seconds\n",
            "Single XGBoost fit took: 26.611528635025024 seconds\n",
            "Single XGBoost fit took: 28.094751119613647 seconds\n",
            "Single XGBoost fit took: 18.98782706260681 seconds\n",
            "Single XGBoost fit took: 13.238300561904907 seconds\n",
            "Single XGBoost fit took: 28.92654252052307 seconds\n",
            "Single XGBoost fit took: 29.424283504486084 seconds\n",
            "Single XGBoost fit took: 24.516014575958252 seconds\n",
            "Single XGBoost fit took: 12.466171503067017 seconds\n",
            "Single XGBoost fit took: 11.443876266479492 seconds\n",
            "[MA] Iter 9 | Evals 119 | Best cost 0.80083\n",
            "Single XGBoost fit took: 12.439241886138916 seconds\n",
            "Single XGBoost fit took: 15.46991491317749 seconds\n",
            "Single XGBoost fit took: 27.15940237045288 seconds\n",
            "Single XGBoost fit took: 24.382993459701538 seconds\n",
            "Single XGBoost fit took: 28.497547388076782 seconds\n",
            "Single XGBoost fit took: 17.228739976882935 seconds\n",
            "Single XGBoost fit took: 12.12272310256958 seconds\n",
            "Single XGBoost fit took: 29.046507120132446 seconds\n",
            "Single XGBoost fit took: 29.41334843635559 seconds\n",
            "Single XGBoost fit took: 23.68902015686035 seconds\n",
            "Single XGBoost fit took: 12.530009984970093 seconds\n",
            "Single XGBoost fit took: 11.939650774002075 seconds\n",
            "[MA] Iter 10 | Evals 131 | Best cost 0.79688\n",
            "\n",
            "✅ Optimization complete\n",
            "Best log10_learning_rate = -0.11796155023717037\n",
            "Best learning_rate = 0.7621464828014912\n",
            "Best cost: 0.796875 => Best F1_micro: 0.203125\n",
            "Elapsed time (s): 2879.3414673805237\n"
          ]
        }
      ],
      "source": [
        "# Remove labels that are constant (all 0 or all 1)\n",
        "sum_per_col = problem['Ytrain'].sum(axis=0)\n",
        "mask = (sum_per_col != 0) & (sum_per_col != problem['Ytrain'].shape[0])\n",
        "\n",
        "problem['Ytrain'] = problem['Ytrain'][:, mask]\n",
        "problem['Yval']   = problem['Yval'][:, mask]\n",
        "\n",
        "print(\"Kept\", mask.sum(), \"labels (removed\", (~mask).sum(), \"constant ones).\")\n",
        "\n",
        "IterPrint = 1\n",
        "MaxIter = 10\n",
        "MaxFuncEvals = 1500\n",
        "curtrial = 1\n",
        "\n",
        "res = MA(problem, IterPrint, MaxIter, MaxFuncEvals, curtrial, initialpop,\n",
        "         mPopSize=mPopSize, fPopSize=fPopSize,\n",
        "         a1=1.0, a2=1.5, a3=1.5, beta=2, dance=5, fl=1,\n",
        "         dance_damp=0.8, fl_damp=0.99, nc=8, gmax=0.8, gmin=0.2, gamma=0.4)\n",
        "\n",
        "namemethod, best_position, best_cost, elapsed_time = res\n",
        "\n",
        "# Decode 3 RF hyperparams\n",
        "best_log10_n_estimators = float(best_position[0])\n",
        "best_max_depth_cont     = float(best_position[1])\n",
        "best_mss_cont           = float(best_position[2])\n",
        "\n",
        "best_n_estimators    = int(np.clip(np.round(10 ** best_log10_n_estimators), 10, 1000))\n",
        "best_max_depth       = int(np.clip(np.round(best_max_depth_cont), 2, 20))\n",
        "best_min_samples_split = int(np.clip(np.round(best_mss_cont), 2, 20))\n",
        "\n",
        "print(\"\\n✅ Optimization complete (RandomForest)\")\n",
        "print(\"Best n_estimators        =\", best_n_estimators)\n",
        "print(\"Best max_depth           =\", best_max_depth)\n",
        "print(\"Best min_samples_split   =\", best_min_samples_split)\n",
        "print(\"Best cost:\", best_cost, \"=> Best F1_micro:\", 1 - best_cost)\n",
        "print(\"Elapsed time (s):\", elapsed_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvS10adaW5Rh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "f8fec737-a354-4ca3-a432-9c52c0ca4066"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "XGBoostError",
          "evalue": "[12:48:41] /workspace/src/objective/regression_obj.cu:119: Check failed: is_valid: base_score must be in (0,1) for the logistic loss.\nStack trace:\n  [bt] (0) /usr/local/lib/python3.12/dist-packages/xgboost/lib/libxgboost.so(+0x2bdf8c) [0x7b92df4bdf8c]\n  [bt] (1) /usr/local/lib/python3.12/dist-packages/xgboost/lib/libxgboost.so(+0x1077654) [0x7b92e0277654]\n  [bt] (2) /usr/local/lib/python3.12/dist-packages/xgboost/lib/libxgboost.so(+0x1077c75) [0x7b92e0277c75]\n  [bt] (3) /usr/local/lib/python3.12/dist-packages/xgboost/lib/libxgboost.so(+0x6d1ab9) [0x7b92df8d1ab9]\n  [bt] (4) /usr/local/lib/python3.12/dist-packages/xgboost/lib/libxgboost.so(+0x6f4db7) [0x7b92df8f4db7]\n  [bt] (5) /usr/local/lib/python3.12/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x77) [0x7b92df3caa67]\n  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.8(+0x7e2e) [0x7b9518852e2e]\n  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.8(+0x4493) [0x7b951884f493]\n  [bt] (8) /usr/lib/python3.12/lib-dynload/_ctypes.cpython-312-x86_64-linux-gnu.so(+0x98c1) [0x7b9519a898c1]\n\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1655830356.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mfinal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trainval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_trainval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Evaluate on test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfitted\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \"\"\"\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0mrouted_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sample_weight\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n\u001b[0m\u001b[1;32m    275\u001b[0m             delayed(_fit_estimator)(\n\u001b[1;32m    276\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mrouted_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1984\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1985\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1986\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1912\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1914\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1915\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36m_fit_estimator\u001b[0;34m(estimator, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1801\u001b[0m             )\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1803\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1804\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1805\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2432\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2433\u001b[0;31m             _check_call(\n\u001b[0m\u001b[1;32m   2434\u001b[0m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[1;32m   2435\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \"\"\"\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mXGBoostError\u001b[0m: [12:48:41] /workspace/src/objective/regression_obj.cu:119: Check failed: is_valid: base_score must be in (0,1) for the logistic loss.\nStack trace:\n  [bt] (0) /usr/local/lib/python3.12/dist-packages/xgboost/lib/libxgboost.so(+0x2bdf8c) [0x7b92df4bdf8c]\n  [bt] (1) /usr/local/lib/python3.12/dist-packages/xgboost/lib/libxgboost.so(+0x1077654) [0x7b92e0277654]\n  [bt] (2) /usr/local/lib/python3.12/dist-packages/xgboost/lib/libxgboost.so(+0x1077c75) [0x7b92e0277c75]\n  [bt] (3) /usr/local/lib/python3.12/dist-packages/xgboost/lib/libxgboost.so(+0x6d1ab9) [0x7b92df8d1ab9]\n  [bt] (4) /usr/local/lib/python3.12/dist-packages/xgboost/lib/libxgboost.so(+0x6f4db7) [0x7b92df8f4db7]\n  [bt] (5) /usr/local/lib/python3.12/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x77) [0x7b92df3caa67]\n  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.8(+0x7e2e) [0x7b9518852e2e]\n  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.8(+0x4493) [0x7b951884f493]\n  [bt] (8) /usr/lib/python3.12/lib-dynload/_ctypes.cpython-312-x86_64-linux-gnu.so(+0x98c1) [0x7b9519a898c1]\n\n"
          ]
        }
      ],
      "source": [
        "best_log10_n_estimators = float(best_position[0])\n",
        "best_max_depth_cont     = float(best_position[1])\n",
        "best_mss_cont           = float(best_position[2])\n",
        "\n",
        "best_n_estimators      = int(np.clip(np.round(10 ** best_log10_n_estimators), 10, 1000))\n",
        "best_max_depth         = int(np.clip(np.round(best_max_depth_cont), 2, 20))\n",
        "best_min_samples_split = int(np.clip(np.round(best_mss_cont), 2, 20))\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "\n",
        "X_trainval = np.vstack([train_embeddings_pca, val_embeddings_pca])\n",
        "Y_trainval = np.vstack([train_targets,       val_targets])\n",
        "\n",
        "final_rf = RandomForestClassifier(\n",
        "    n_estimators=best_n_estimators,\n",
        "    max_depth=best_max_depth,\n",
        "    min_samples_split=best_min_samples_split,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "final_model = MultiOutputClassifier(final_rf)\n",
        "\n",
        "final_model.fit(X_trainval, Y_trainval)\n",
        "\n",
        "test_preds = final_model.predict(test_embeddings_pca)\n",
        "\n",
        "print(\"\\n📊 Test Metrics (RandomForest):\")\n",
        "print(\"Test F1 macro:\", f1_score(test_targets, test_preds, average='macro'))\n",
        "print(\"Test F1 micro:\", f1_score(test_targets, test_preds, average='micro'))\n",
        "print(\"Hamming Loss:\", hamming_loss(test_targets, test_preds))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQqBQVhiW_7W"
      },
      "outputs": [],
      "source": [
        "min_len = min(len(history_iter),\n",
        "              len(history_log10_n_estimators),\n",
        "              len(history_max_depth),\n",
        "              len(history_min_samples_split),\n",
        "              len(history_f1micro))\n",
        "\n",
        "history_iter               = history_iter[:min_len]\n",
        "history_log10_n_estimators = history_log10_n_estimators[:min_len]\n",
        "history_max_depth          = history_max_depth[:min_len]\n",
        "history_min_samples_split  = history_min_samples_split[:min_len]\n",
        "history_f1micro            = history_f1micro[:min_len]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdfRI3BEXMIy"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "\n",
        "# ----- Plot 1: log10(n_estimators) -----\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(history_iter, history_log10_n_estimators, marker='o')\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"log10(n_estimators)\")\n",
        "plt.title(\"Evolution of n_estimators\")\n",
        "plt.grid(True)\n",
        "\n",
        "# ----- Plot 2: max_depth -----\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(history_iter, history_max_depth, marker='o')\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"max_depth\")\n",
        "plt.title(\"Evolution of max_depth\")\n",
        "plt.grid(True)\n",
        "\n",
        "# ----- Plot 3: min_samples_split -----\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(history_iter, history_min_samples_split, marker='o')\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"min_samples_split\")\n",
        "plt.title(\"Evolution of min_samples_split\")\n",
        "plt.grid(True)\n",
        "\n",
        "# ----- Plot 4: F1-micro -----\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(history_iter, history_f1micro, marker='o')\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"F1-micro\")\n",
        "plt.title(\"F1-micro Progression\")\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}